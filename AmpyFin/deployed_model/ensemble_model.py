# ensemble_model.py
import pandas as pd
import numpy as np
import os
import logging
import joblib
import platform
import json
import traceback
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional, Union, Set
from collections import defaultdict

# FORCE ENABLE DETAILED LOGGING - Using ERROR level to ensure visibility
logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)  # Force logger to ERROR level

# Machine learning imports
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import f1_score, fbeta_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.model_selection import TimeSeriesSplit, cross_val_predict, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, clone
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Print a very obvious startup message
print("\n" + "!"*80)
print("üö® ENSEMBLE MODEL RELOAD TRIGGERED üö®")
print("!"*80 + "\n")

# Deep learning imports with improved handling
TENSORFLOW_AVAILABLE = False
try:
    import tensorflow as tf
    from tensorflow import keras
    
    # Access components through the keras object
    Sequential = keras.Sequential
    Dense = keras.layers.Dense
    LSTM = keras.layers.LSTM
    GRU = keras.layers.GRU
    Bidirectional = keras.layers.Bidirectional
    Conv1D = keras.layers.Conv1D
    Dropout = keras.layers.Dropout
    BatchNormalization = keras.layers.BatchNormalization
    Attention = keras.layers.Attention
    LayerNormalization = keras.layers.LayerNormalization
    Add = keras.layers.Add
    Adam = keras.optimizers.Adam
    EarlyStopping = keras.callbacks.EarlyStopping
    ModelCheckpoint = keras.callbacks.ModelCheckpoint
    ReduceLROnPlateau = keras.callbacks.ReduceLROnPlateau
    TensorBoard = keras.callbacks.TensorBoard
    load_model = keras.models.load_model
    save_model = keras.models.save_model
    
    # Enable mixed precision for faster training on compatible hardware
    try:
        mixed_precision = tf.keras.mixed_precision
        policy = mixed_precision.Policy('mixed_float16')
        mixed_precision.set_global_policy(policy)
        logger.info("Mixed precision enabled with policy: mixed_float16")
    except:
        logger.info("Mixed precision not enabled")
    
    TENSORFLOW_AVAILABLE = True
    TF_VERSION = tf.__version__
    logger.info(f"TensorFlow {TF_VERSION} successfully imported")
except ImportError as e:
    # More helpful error message
    if platform.processor() == 'arm':
        logger.warning("TensorFlow not found. For Apple Silicon Macs, install with: pip install tensorflow-macos tensorflow-metal")
    else:
        logger.warning("TensorFlow not found. Install with: pip install tensorflow")
    logger.info(f"Deep learning features will be disabled. Error: {e}")


class EnsembleModelManager:
    """
    Advanced ensemble model that combines multiple ML models using stacking approach.
    
    Features:
    - Multiple model types (tree-based, neural networks, and traditional ML)
    - Time series cross-validation
    - Feature importance tracking
    - Automatic hyperparameter tuning
    - Robust error handling
    - Detailed performance metrics
    - Enhanced LSTM architecture with attention
    """
    
    def __init__(self, model_dir="./ensemble_models", n_regimes=3):
        """
        Initialize the ensemble model manager.
        
        Args:
            model_dir: Directory to save/load model files
            n_regimes: Number of market regimes to consider (for regime-specific models)
        """
        print("üîÑ INIT: EnsembleModelManager initialization started üîÑ")
        logger.error("üîÑ INIT: EnsembleModelManager initialization started üîÑ")
        self.model_dir = model_dir
        self.n_regimes = n_regimes
        
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            
        # Sub-models in the ensemble
        self.models = {}
        self.scalers = {}
        self.feature_subsets = {}
        
        # Meta-learner model
        self.meta_model = None
        self.meta_scaler = None
        self.meta_feature_names = []  # Store expected meta feature names
        
        # Track feature importance across all models
        self.feature_importance = {}
        
        # Feature columns information
        self.all_feature_cols = []
        
        # Sub-model performance metrics
        self.model_metrics = {}
        
        # Track trained epochs for neural network models
        self.model_epochs = {}
        
        # Regime-specific models
        self.regime_specific_models = {}
        
        # LSTM model (TensorFlow required)
        self.lstm_model = None
        self.lstm_scaler = None
        self.lstm_sequence_length = 10  # Default sequence length for LSTM
        self.lstm_feature_cols = []  # Store feature columns used by LSTM
        
        # Configuration for model training
        self.config = {
            'random_state': 42,
            'test_size': 0.2,
            'cv_folds': 5,
            'hyperparameter_tuning': False,
            'early_stopping_patience': 10,
            'lstm_batch_size': 32,
            'lstm_epochs': 50,
            'meta_learner_type': 'xgboost',  # 'xgboost', 'lightgbm', 'randomforest', 'voting'
            'use_attention': True,           # Use attention mechanism in LSTM
            'use_feature_selection': True,   # Perform automatic feature selection
        }
        
        # Load models if they exist
        self.load_models()
        print("üîÑ INIT: EnsembleModelManager initialization completed üîÑ")
        logger.error("üîÑ INIT: EnsembleModelManager initialization completed üîÑ")
    
    def debug_meta_features(self):
        """
        Extensive debugging method to diagnose meta-features and model issues.
        Call this before making predictions to identify potential problems.
        """
        print("=" * 50)
        print("üîç ENSEMBLE MODEL DEBUG INFORMATION üîç")
        print("=" * 50)
        logger.error("=" * 50)
        logger.error("üîç ENSEMBLE MODEL DEBUG INFORMATION üîç")
        logger.error("=" * 50)
        
        # Check meta-model
        print("Meta-model status:")
        logger.error("Meta-model status:")
        print(f"  Meta-model exists: {self.meta_model is not None}")
        logger.error(f"  Meta-model exists: {self.meta_model is not None}")
        print(f"  Meta-scaler exists: {self.meta_scaler is not None}")
        logger.error(f"  Meta-scaler exists: {self.meta_scaler is not None}")
        if hasattr(self.meta_model, 'feature_names_in_'):
            print(f"  Meta-model feature_names_in_: {self.meta_model.feature_names_in_}")
            logger.error(f"  Meta-model feature_names_in_: {self.meta_model.feature_names_in_}")
        
        # Check meta feature names
        print("Meta feature names:")
        logger.error("Meta feature names:")
        print(f"  self.meta_feature_names: {self.meta_feature_names}")
        logger.error(f"  self.meta_feature_names: {self.meta_feature_names}")
        
        # Check all base models
        print("Base models:")
        logger.error("Base models:")
        for model_name, model in self.models.items():
            features = self.feature_subsets.get(model_name, [])
            print(f"  {model_name}: {len(features)} features")
            logger.error(f"  {model_name}: {len(features)} features")
            
        # Check LSTM model
        print("LSTM model status:")
        logger.error("LSTM model status:")
        print(f"  LSTM model exists: {self.lstm_model is not None}")
        logger.error(f"  LSTM model exists: {self.lstm_model is not None}")
        print(f"  LSTM scaler exists: {self.lstm_scaler is not None}")
        logger.error(f"  LSTM scaler exists: {self.lstm_scaler is not None}")
        print(f"  LSTM feature columns: {len(self.lstm_feature_cols)} features")
        logger.error(f"  LSTM feature columns: {len(self.lstm_feature_cols)} features")
        
        # Generate meta features for a sample prediction
        print("Generating sample meta features:")
        logger.error("Generating sample meta features:")
        # Create a tiny sample DataFrame with the expected features
        if self.all_feature_cols:
            sample_data = {col: [0.0] for col in self.all_feature_cols}
            sample_df = pd.DataFrame(sample_data)
            model_predictions = {}
            
            # Get predictions from each model for the sample
            for model_name, model in self.models.items():
                features = self.feature_subsets.get(model_name, [])
                if features:
                    model_predictions[f"{model_name}_prob"] = 0.5
            
            # Always add lstm_prob
            model_predictions['lstm_prob'] = 0.5
            
            # Log meta features
            print(f"  Sample meta features: {list(model_predictions.keys())}")
            logger.error(f"  Sample meta features: {list(model_predictions.keys())}")
            
            # Check if the expected meta feature columns match what would be generated
            if self.meta_feature_names:
                missing = [f for f in self.meta_feature_names if f not in model_predictions]
                extra = [f for f in model_predictions if f not in self.meta_feature_names]
                
                if missing:
                    print(f"  üö® CRITICAL: Missing expected meta features: {missing}")
                    logger.error(f"  üö® CRITICAL: Missing expected meta features: {missing}")
                if extra:
                    print(f"  ‚ö†Ô∏è WARNING: Extra meta features not in model: {extra}")
                    logger.error(f"  ‚ö†Ô∏è WARNING: Extra meta features not in model: {extra}")
            
        print("=" * 50)
        logger.error("=" * 50)
        
        print("Debug meta-features completed! Check logs for details.")
    
    def load_models(self):
        """Load all models if they exist."""
        try:
            # DEBUGGING
            print("üîÑ LOAD: load_models called üîÑ")
            logger.error("üîÑ LOAD: load_models called üîÑ")
            print(f"üîÑ LOAD: Looking for models in: {self.model_dir}")
            logger.error(f"üîÑ LOAD: Looking for models in: {self.model_dir}")
            
            # Check if directory exists
            if not os.path.exists(self.model_dir):
                print(f"üîÑ LOAD: Model directory {self.model_dir} not found. Will create new models.")
                logger.error(f"üîÑ LOAD: Model directory {self.model_dir} not found. Will create new models.")
                return False
                
            # Check for model registry file
            registry_path = os.path.join(self.model_dir, "model_registry.json")
            if not os.path.exists(registry_path):
                print("üîÑ LOAD: Model registry not found. Will create new models.")
                logger.error("üîÑ LOAD: Model registry not found. Will create new models.")
                return False
                
            # Load model registry using json for better compatibility
            with open(registry_path, 'r') as f:
                registry = json.load(f)
            print(f"üîÑ LOAD: Successfully loaded registry from {registry_path}")
            logger.error(f"üîÑ LOAD: Successfully loaded registry from {registry_path}")
            
            # Load configuration if available
            if 'config' in registry:
                self.config.update(registry['config'])
            
            # Load feature information
            if 'all_feature_cols' in registry:
                self.all_feature_cols = registry['all_feature_cols']
                print(f"üîÑ LOAD: Loaded {len(self.all_feature_cols)} feature columns")
                logger.error(f"üîÑ LOAD: Loaded {len(self.all_feature_cols)} feature columns")
            
            # Load meta feature names if available
            if 'meta_feature_names' in registry:
                self.meta_feature_names = registry['meta_feature_names']
                print(f"üîÑ LOAD: Loaded meta_feature_names: {self.meta_feature_names}")
                logger.error(f"üîÑ LOAD: Loaded meta_feature_names: {self.meta_feature_names}")
            
            # Load each sub-model based on registry
            for model_name, model_info in registry.get('models', {}).items():
                model_path = os.path.join(self.model_dir, f"{model_name}.joblib")
                scaler_path = os.path.join(self.model_dir, f"{model_name}_scaler.joblib")
                
                if os.path.exists(model_path) and os.path.exists(scaler_path):
                    self.models[model_name] = joblib.load(model_path)
                    self.scalers[model_name] = joblib.load(scaler_path)
                    self.feature_subsets[model_name] = model_info.get('features', [])
                    print(f"üîÑ LOAD: Loaded model {model_name}")
                    logger.error(f"üîÑ LOAD: Loaded model {model_name}")
            
            # Load meta-model if it exists
            meta_model_path = os.path.join(self.model_dir, "meta_model.joblib")
            meta_scaler_path = os.path.join(self.model_dir, "meta_scaler.joblib")
            
            if os.path.exists(meta_model_path) and os.path.exists(meta_scaler_path):
                self.meta_model = joblib.load(meta_model_path)
                self.meta_scaler = joblib.load(meta_scaler_path)
                print("üîÑ LOAD: Loaded meta-model")
                logger.error("üîÑ LOAD: Loaded meta-model")
                
                # Try to access feature names
                if hasattr(self.meta_model, 'feature_names_in_'):
                    print(f"üîÑ LOAD: Meta-model feature_names_in_: {self.meta_model.feature_names_in_}")
                    logger.error(f"üîÑ LOAD: Meta-model feature_names_in_: {self.meta_model.feature_names_in_}")
            
            # Load feature importance if available
            importance_path = os.path.join(self.model_dir, "feature_importance.joblib")
            if os.path.exists(importance_path):
                self.feature_importance = joblib.load(importance_path)
            
            # Load model metrics if available
            metrics_path = os.path.join(self.model_dir, "model_metrics.joblib")
            if os.path.exists(metrics_path):
                self.model_metrics = joblib.load(metrics_path)
            
            # Load LSTM model if TensorFlow is available
            if TENSORFLOW_AVAILABLE:
                # Try multiple possible LSTM model paths
                lstm_paths = [
                    os.path.join(self.model_dir, "lstm_model.keras"),
                    os.path.join(self.model_dir, "lstm_model.h5")
                ]
                
                # Track if LSTM model was successfully loaded
                lstm_model_loaded = False
                for lstm_path in lstm_paths:
                    if os.path.exists(lstm_path):
                        try:
                            self.lstm_model = load_model(lstm_path)
                            print(f"üîÑ LOAD: Loaded LSTM model from {lstm_path}")
                            logger.error(f"üîÑ LOAD: Loaded LSTM model from {lstm_path}")
                            lstm_model_loaded = True
                            break
                        except Exception as e:
                            print(f"üîÑ LOAD: Error loading LSTM model from {lstm_path}: {e}")
                            logger.error(f"üîÑ LOAD: Error loading LSTM model from {lstm_path}: {e}")
                
                # Load LSTM scaler
                lstm_scaler_path = os.path.join(self.model_dir, "lstm_scaler.joblib")
                if os.path.exists(lstm_scaler_path):
                    self.lstm_scaler = joblib.load(lstm_scaler_path)
                    print("üîÑ LOAD: Loaded LSTM scaler")
                    logger.error("üîÑ LOAD: Loaded LSTM scaler")
                
                # Load LSTM feature columns
                if 'lstm_feature_cols' in registry:
                    self.lstm_feature_cols = registry['lstm_feature_cols']
                
                # Load LSTM sequence length
                if 'lstm_sequence_length' in registry:
                    self.lstm_sequence_length = registry['lstm_sequence_length']
            
            print(f"üîÑ LOAD: Successfully loaded {len(self.models)} models and meta-model")
            logger.error(f"üîÑ LOAD: Successfully loaded {len(self.models)} models and meta-model")
            return True
            
        except Exception as e:
            print(f"üîÑ LOAD ERROR: Error loading models: {e}")
            print(f"üîÑ LOAD ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üîÑ LOAD ERROR: Error loading models: {e}")
            logger.error(f"üîÑ LOAD ERROR TRACEBACK:\n{traceback.format_exc()}")
            # Reset to empty state
            self.models = {}
            self.scalers = {}
            self.feature_subsets = {}
            self.meta_model = None
            self.meta_scaler = None
            return False
    
    def save_models(self):
        """Save all models and related artifacts."""
        try:
            print("üíæ SAVE: save_models called üíæ")
            logger.error("üíæ SAVE: save_models called üíæ")
            
            # Ensure directory exists
            if not os.path.exists(self.model_dir):
                os.makedirs(self.model_dir)
            
            # Save each sub-model
            for model_name, model in self.models.items():
                model_path = os.path.join(self.model_dir, f"{model_name}.joblib")
                scaler_path = os.path.join(self.model_dir, f"{model_name}_scaler.joblib")
                
                joblib.dump(model, model_path)
                joblib.dump(self.scalers[model_name], scaler_path)
                print(f"üíæ SAVE: Saved model {model_name}")
                logger.error(f"üíæ SAVE: Saved model {model_name}")
            
            # Save meta-model
            if self.meta_model is not None:
                meta_model_path = os.path.join(self.model_dir, "meta_model.joblib")
                meta_scaler_path = os.path.join(self.model_dir, "meta_scaler.joblib")
                
                joblib.dump(self.meta_model, meta_model_path)
                joblib.dump(self.meta_scaler, meta_scaler_path)
                print("üíæ SAVE: Saved meta-model")
                logger.error("üíæ SAVE: Saved meta-model")
            
            # Save feature importance
            if self.feature_importance:
                importance_path = os.path.join(self.model_dir, "feature_importance.joblib")
                joblib.dump(self.feature_importance, importance_path)
                print("üíæ SAVE: Saved feature importance")
                logger.error("üíæ SAVE: Saved feature importance")
            
            # Save model metrics
            if self.model_metrics:
                metrics_path = os.path.join(self.model_dir, "model_metrics.joblib")
                joblib.dump(self.model_metrics, metrics_path)
                print("üíæ SAVE: Saved model metrics")
                logger.error("üíæ SAVE: Saved model metrics")
            
            # Save LSTM model if available with proper extension
            if TENSORFLOW_AVAILABLE and self.lstm_model is not None:
                # First try .keras extension (recommended for TF 2.x)
                try:
                    lstm_path = os.path.join(self.model_dir, "lstm_model.keras")
                    self.lstm_model.save(lstm_path, save_format='keras')
                    print(f"üíæ SAVE: LSTM model saved to {lstm_path} in Keras format")
                    logger.error(f"üíæ SAVE: LSTM model saved to {lstm_path} in Keras format")
                except Exception as e:
                    print(f"üíæ SAVE: Error saving with .keras extension: {e}")
                    logger.error(f"üíæ SAVE: Error saving with .keras extension: {e}")
                    try:
                        # Fall back to .h5 extension
                        lstm_path = os.path.join(self.model_dir, "lstm_model.h5") 
                        self.lstm_model.save(lstm_path, save_format='h5')
                        print(f"üíæ SAVE: LSTM model saved to {lstm_path} in H5 format")
                        logger.error(f"üíæ SAVE: LSTM model saved to {lstm_path} in H5 format")
                    except Exception as e2:
                        print(f"üíæ SAVE: Error saving LSTM model with .h5 extension: {e2}")
                        logger.error(f"üíæ SAVE: Error saving LSTM model with .h5 extension: {e2}")
            
            # Save LSTM scaler if available
            if self.lstm_scaler is not None:
                lstm_scaler_path = os.path.join(self.model_dir, "lstm_scaler.joblib")
                joblib.dump(self.lstm_scaler, lstm_scaler_path)
                print("üíæ SAVE: LSTM scaler saved")
                logger.error("üíæ SAVE: LSTM scaler saved")
            
            # Create and save model registry in JSON format for better compatibility
            registry = {
                'last_updated': datetime.now().isoformat(),
                'config': self.config,
                'all_feature_cols': self.all_feature_cols,
                'meta_feature_names': self.meta_feature_names,
                'lstm_feature_cols': self.lstm_feature_cols,
                'lstm_sequence_length': self.lstm_sequence_length,
                'models': {
                    model_name: {
                        'type': type(model).__name__,
                        'features': self.feature_subsets.get(model_name, [])
                    } for model_name, model in self.models.items()
                },
                'meta_model': {
                    'type': type(self.meta_model).__name__ if self.meta_model else None,
                }
            }
            
            # Save registry as JSON
            registry_path = os.path.join(self.model_dir, "model_registry.json")
            with open(registry_path, 'w') as f:
                json.dump(registry, f, indent=2)
            
            print(f"üíæ SAVE: Successfully saved all models to {self.model_dir}")
            logger.error(f"üíæ SAVE: Successfully saved all models to {self.model_dir}")
            
            return True
            
        except Exception as e:
            print(f"üíæ SAVE ERROR: Error saving models: {e}")
            print(f"üíæ SAVE ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üíæ SAVE ERROR: Error saving models: {e}")
            logger.error(f"üíæ SAVE ERROR TRACEBACK:\n{traceback.format_exc()}")
            return False
    
    def train_ensemble(self, data_frames, test_size=0.2, random_state=42, deep_learning=True):
        """
        Train the full ensemble of models including base models and meta-learner.
        
        Args:
            data_frames: List of pandas DataFrames with training data
            test_size: Fraction of data to use for testing
            random_state: Random seed for reproducibility
            deep_learning: Whether to include deep learning models
            
        Returns:
            dict: Dictionary with training metrics
        """
        print("üèãÔ∏è TRAIN: train_ensemble called üèãÔ∏è")
        logger.error("üèãÔ∏è TRAIN: train_ensemble called üèãÔ∏è")
        
        if not data_frames:
            print("üèãÔ∏è TRAIN: No data provided for training")
            logger.error("üèãÔ∏è TRAIN: No data provided for training")
            return {'success': False, 'message': 'No data provided'}
        
        try:
            # Update configuration
            self.config['test_size'] = test_size
            self.config['random_state'] = random_state
            
            # Combine all dataframes
            all_data = pd.concat(data_frames, ignore_index=True)
            print(f"üèãÔ∏è TRAIN: Combined {len(data_frames)} dataframes, total rows: {len(all_data)}")
            logger.error(f"üèãÔ∏è TRAIN: Combined {len(data_frames)} dataframes, total rows: {len(all_data)}")
            
            # Make sure we have the Label column
            if 'Label' not in all_data.columns:
                print("üèãÔ∏è TRAIN: Label column not found in training data")
                logger.error("üèãÔ∏è TRAIN: Label column not found in training data")
                return {'success': False, 'message': 'Label column not found'}
            
            # Get all feature columns (excluding non-features)
            exclude_cols = ['Label', 'Symbol', 'Timestamp', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']
            all_feature_cols = [col for col in all_data.columns if col not in exclude_cols]
            
            # Store feature columns for future reference
            self.all_feature_cols = all_feature_cols
            print(f"üèãÔ∏è TRAIN: Found {len(all_feature_cols)} feature columns")
            logger.error(f"üèãÔ∏è TRAIN: Found {len(all_feature_cols)} feature columns")
            
            if not all_feature_cols:
                print("üèãÔ∏è TRAIN: No feature columns found")
                logger.error("üèãÔ∏è TRAIN: No feature columns found")
                return {'success': False, 'message': 'No feature columns found'}
            
            # Sort by time first to avoid data leakage
            if 'Timestamp' in all_data.columns:
                all_data = all_data.sort_values('Timestamp')
                print("üèãÔ∏è TRAIN: Sorted data by Timestamp")
                logger.error("üèãÔ∏è TRAIN: Sorted data by Timestamp")
            elif 'Date' in all_data.columns:
                all_data = all_data.sort_values('Date')
                print("üèãÔ∏è TRAIN: Sorted data by Date")
                logger.error("üèãÔ∏è TRAIN: Sorted data by Date")
            
            # Use the last 20% as test set (time-aware split for financial data)
            train_idx = int(len(all_data) * (1 - test_size))
            train_data = all_data.iloc[:train_idx]
            test_data = all_data.iloc[train_idx:]
            
            print(f"üèãÔ∏è TRAIN: Train set: {len(train_data)} samples, Test set: {len(test_data)} samples")
            logger.error(f"üèãÔ∏è TRAIN: Train set: {len(train_data)} samples, Test set: {len(test_data)} samples")
            
            # Extract features and labels
            X_train = train_data[all_feature_cols]
            y_train = train_data['Label']
            
            X_test = test_data[all_feature_cols]
            y_test = test_data['Label']
            
            # Check for class imbalance and log it
            class_distribution = dict(train_data['Label'].value_counts())
            print(f"üèãÔ∏è TRAIN: Class distribution in training data: {class_distribution}")
            logger.error(f"üèãÔ∏è TRAIN: Class distribution in training data: {class_distribution}")
            
            # Detect and handle outliers/anomalies
            if self._detect_outliers(X_train):
                print("üèãÔ∏è TRAIN: Outliers detected in training data, using RobustScaler")
                logger.error("üèãÔ∏è TRAIN: Outliers detected in training data, using RobustScaler")
                scaler_type = "robust"
            else:
                scaler_type = "standard"
                print("üèãÔ∏è TRAIN: No significant outliers detected, using StandardScaler")
                logger.error("üèãÔ∏è TRAIN: No significant outliers detected, using StandardScaler")
            
            # Train multiple base models with different feature subsets
            print("üèãÔ∏è TRAIN: Training base models...")
            logger.error("üèãÔ∏è TRAIN: Training base models...")
            base_models_trained = self._train_base_models(X_train, y_train, X_test, y_test, all_feature_cols, scaler_type)
            
            if not base_models_trained:
                print("üèãÔ∏è TRAIN: Failed to train base models")
                logger.error("üèãÔ∏è TRAIN: Failed to train base models")
                return {'success': False, 'message': 'Base model training failed'}
            
            # Train LSTM model if enabled and TensorFlow is available
            lstm_trained = False
            if deep_learning and TENSORFLOW_AVAILABLE:
                try:
                    print("üèãÔ∏è TRAIN: Training LSTM model...")
                    logger.error("üèãÔ∏è TRAIN: Training LSTM model...")
                    lstm_trained = self._train_lstm_model(all_data, all_feature_cols)
                    print(f"üèãÔ∏è TRAIN: LSTM model training {'successful' if lstm_trained else 'failed'}")
                    logger.error(f"üèãÔ∏è TRAIN: LSTM model training {'successful' if lstm_trained else 'failed'}")
                except Exception as e:
                    print(f"üèãÔ∏è TRAIN: Error training LSTM model: {e}")
                    print(f"üèãÔ∏è TRAIN: Error traceback:\n{traceback.format_exc()}")
                    logger.error(f"üèãÔ∏è TRAIN: Error training LSTM model: {e}")
                    logger.error(f"üèãÔ∏è TRAIN: Error traceback:\n{traceback.format_exc()}")
                    lstm_trained = False
            
            # Create meta-features by getting predictions from base models
            print("üèãÔ∏è TRAIN: Creating meta-features...")
            logger.error("üèãÔ∏è TRAIN: Creating meta-features...")
            meta_features_train, meta_features_test = self._create_meta_features(X_train, X_test)
            
            # Store meta feature names for future reference
            if not meta_features_train.empty:
                self.meta_feature_names = meta_features_train.columns.tolist()
                print(f"üèãÔ∏è TRAIN: Meta-features: {self.meta_feature_names}")
                logger.error(f"üèãÔ∏è TRAIN: Meta-features: {self.meta_feature_names}")
            
            # Train meta-learner
            print("üèãÔ∏è TRAIN: Training meta-learner...")
            logger.error("üèãÔ∏è TRAIN: Training meta-learner...")
            meta_success = self._train_meta_learner(meta_features_train, y_train, meta_features_test, y_test)
            
            if not meta_success:
                print("üèãÔ∏è TRAIN: Meta-learner training failed, will use base models only")
                logger.error("üèãÔ∏è TRAIN: Meta-learner training failed, will use base models only")
            
            # Calculate ensemble metrics
            print("üèãÔ∏è TRAIN: Evaluating ensemble...")
            logger.error("üèãÔ∏è TRAIN: Evaluating ensemble...")
            ensemble_metrics = self._evaluate_ensemble(X_test, y_test)
            
            # Save all models
            print("üèãÔ∏è TRAIN: Saving models...")
            logger.error("üèãÔ∏è TRAIN: Saving models...")
            self.save_models()
            
            print(f"üèãÔ∏è TRAIN: Ensemble training completed with f2_score: {ensemble_metrics.get('f2_score', 0):.4f}")
            logger.error(f"üèãÔ∏è TRAIN: Ensemble training completed with f2_score: {ensemble_metrics.get('f2_score', 0):.4f}")
            
            return {
                'success': True,
                'message': 'Ensemble training completed successfully',
                'metrics': ensemble_metrics,
                'lstm_trained': lstm_trained,
                'feature_importance': self._get_top_features(10)
            }
            
        except Exception as e:
            print(f"üèãÔ∏è TRAIN ERROR: Error training ensemble: {e}")
            print(f"üèãÔ∏è TRAIN ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üèãÔ∏è TRAIN ERROR: Error training ensemble: {e}")
            logger.error(f"üèãÔ∏è TRAIN ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'success': False, 'message': f'Error: {str(e)}'}
    
    def _detect_outliers(self, X):
        """
        Detect if there are significant outliers in the data.
        
        Args:
            X: Feature matrix
            
        Returns:
            bool: True if outliers are detected
        """
        try:
            print("üîç OUTLIER: _detect_outliers called")
            logger.error("üîç OUTLIER: _detect_outliers called")
            
            # Simple outlier detection by looking at skew
            numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns
            if len(numerical_cols) == 0:
                print("üîç OUTLIER: No numerical columns found")
                logger.error("üîç OUTLIER: No numerical columns found")
                return False
                
            # Check skewness
            skew = X[numerical_cols].skew(numeric_only=True)
            high_skew = (abs(skew) > 3).sum()
            
            # If more than 20% of features have high skew, consider the data to have outliers
            result = high_skew > 0.2 * len(numerical_cols)
            print(f"üîç OUTLIER: Detected {high_skew} columns with high skew out of {len(numerical_cols)}")
            logger.error(f"üîç OUTLIER: Detected {high_skew} columns with high skew out of {len(numerical_cols)}")
            print(f"üîç OUTLIER: Outliers detected: {result}")
            logger.error(f"üîç OUTLIER: Outliers detected: {result}")
            return result
        except Exception as e:
            print(f"üîç OUTLIER ERROR: {e}")
            logger.error(f"üîç OUTLIER ERROR: {e}")
            return False
    
    def _train_base_models(self, X_train, y_train, X_test, y_test, all_feature_cols, scaler_type="standard"):
        """
        Train the base models with different feature subsets.
        
        Args:
            X_train: Training features
            y_train: Training labels
            X_test: Test features
            y_test: Test labels
            all_feature_cols: All available feature columns
            scaler_type: Type of scaler to use ('standard' or 'robust')
            
        Returns:
            bool: Success or failure
        """
        try:
            print("üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: _train_base_models called")
            logger.error("üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: _train_base_models called")
            
            # Define feature subsets (domain knowledge based)
            feature_subsets = self._create_feature_subsets(all_feature_cols)
            print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Created {len(feature_subsets)} feature subsets")
            logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Created {len(feature_subsets)} feature subsets")
            
            # Store feature subsets
            self.feature_subsets = feature_subsets
            
            # Define models to train for each feature subset
            model_configs = [
                # XGBoost models
                {'name': 'xgb_all', 'model': XGBClassifier(
                    use_label_encoder=False, 
                    eval_metric='logloss', 
                    random_state=self.config['random_state'],
                    n_estimators=200,
                    learning_rate=0.05,
                    max_depth=6
                ), 'subset': 'all_features'},
                
                # LightGBM models
                {'name': 'lgb_all', 'model': LGBMClassifier(
                    random_state=self.config['random_state'],
                    n_estimators=200,
                    learning_rate=0.05
                ), 'subset': 'all_features'},
                
                # Random Forest models
                {'name': 'rf_all', 'model': RandomForestClassifier(
                    n_estimators=200, 
                    max_depth=8,
                    random_state=self.config['random_state']
                ), 'subset': 'all_features'},
                
                # Gradient Boosting models
                {'name': 'gb_tech', 'model': GradientBoostingClassifier(
                    n_estimators=150,
                    learning_rate=0.05,
                    max_depth=5,
                    random_state=self.config['random_state']
                ), 'subset': 'technical'},
                
                # Specialized XGBoost models
                {'name': 'xgb_tech', 'model': XGBClassifier(
                    use_label_encoder=False, 
                    eval_metric='logloss', 
                    random_state=self.config['random_state']
                ), 'subset': 'technical'},
                
                {'name': 'xgb_vol', 'model': XGBClassifier(
                    use_label_encoder=False, 
                    eval_metric='logloss', 
                    random_state=self.config['random_state']
                ), 'subset': 'volatility'},
                
                # Specialized LightGBM models
                {'name': 'lgb_mom', 'model': LGBMClassifier(
                    random_state=self.config['random_state']
                ), 'subset': 'momentum'},
                
                {'name': 'lgb_price', 'model': LGBMClassifier(
                    random_state=self.config['random_state']
                ), 'subset': 'price_action'},
                
                # More Random Forest models
                {'name': 'rf_price', 'model': RandomForestClassifier(
                    n_estimators=150, 
                    random_state=self.config['random_state']
                ), 'subset': 'price_action'},
                
                # More XGBoost specialized models
                {'name': 'xgb_flow', 'model': XGBClassifier(
                    use_label_encoder=False, 
                    eval_metric='logloss', 
                    random_state=self.config['random_state']
                ), 'subset': 'order_flow'}
            ]
            
            # Ensure we only use feature subsets that exist
            model_configs = [cfg for cfg in model_configs if cfg['subset'] in feature_subsets and len(feature_subsets[cfg['subset']]) >= 5]
            print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Will train {len(model_configs)} base models")
            logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Will train {len(model_configs)} base models")
            
            # Train each model
            for cfg in model_configs:
                subset_name = cfg['subset']
                model_name = cfg['name']
                model = cfg['model']
                
                # Get feature subset
                features = feature_subsets[subset_name]
                
                if not features:
                    print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: No features for subset {subset_name}, skipping model {model_name}")
                    logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: No features for subset {subset_name}, skipping model {model_name}")
                    continue
                
                print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Training {model_name} with {len(features)} features from {subset_name}")
                logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Training {model_name} with {len(features)} features from {subset_name}")
                
                # Extract feature subset
                X_train_subset = X_train[features]
                X_test_subset = X_test[features]
                
                # Create and fit scaler
                if scaler_type == "robust":
                    scaler = RobustScaler()
                else:
                    scaler = StandardScaler()
                
                X_train_scaled = scaler.fit_transform(X_train_subset)
                X_test_scaled = scaler.transform(X_test_subset)
                
                # Train model with early stopping if supported
                if hasattr(model, 'fit') and ('xgb' in model_name or 'lgb' in model_name):
                    # For XGBoost and LightGBM, use early stopping with a validation set
                    valid_size = 0.2
                    valid_samples = int(X_train_scaled.shape[0] * valid_size)
                    
                    # Split into train and validation
                    X_train_model = X_train_scaled[:-valid_samples]
                    X_valid = X_train_scaled[-valid_samples:]
                    y_train_model = y_train.iloc[:-valid_samples]
                    y_valid = y_train.iloc[-valid_samples:]
                    
                    # Create eval set
                    eval_set = [(X_valid, y_valid)]
                    
                    # Fit with early stopping
                    if 'xgb' in model_name:
                        model.fit(
                            X_train_model, y_train_model,
                            eval_set=eval_set,
                            early_stopping_rounds=20,
                            verbose=False
                        )
                    elif 'lgb' in model_name:
                        model.fit(
                            X_train_model, y_train_model,
                            eval_set=eval_set,
                            early_stopping_rounds=20,
                            verbose=False
                        )
                    else:
                        model.fit(X_train_scaled, y_train)
                else:
                    # Standard fit for other models
                    model.fit(X_train_scaled, y_train)
                
                # Store model and scaler
                self.models[model_name] = model
                self.scalers[model_name] = scaler
                self.feature_subsets[model_name] = features
                print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Model {model_name} trained successfully")
                logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Model {model_name} trained successfully")
                
                # Calculate and store metrics
                y_pred = model.predict(X_test_scaled)
                
                metrics = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'f1_score': f1_score(y_test, y_pred),
                    'f2_score': fbeta_score(y_test, y_pred, beta=2),
                    'precision': precision_score(y_test, y_pred),
                    'recall': recall_score(y_test, y_pred),
                }
                
                # Add AUC if model supports predict_proba
                if hasattr(model, 'predict_proba'):
                    try:
                        proba = model.predict_proba(X_test_scaled)[:, 1]
                        metrics['auc'] = roc_auc_score(y_test, proba)
                    except:
                        metrics['auc'] = 0.5
                else:
                    metrics['auc'] = 0.5
                
                self.model_metrics[model_name] = metrics
                
                print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Model {model_name} - f2_score: {metrics['f2_score']:.4f}, AUC: {metrics['auc']:.4f}")
                logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Model {model_name} - f2_score: {metrics['f2_score']:.4f}, AUC: {metrics['auc']:.4f}")
                
                # Store feature importances if available
                if hasattr(model, 'feature_importances_'):
                    self.feature_importance[model_name] = dict(zip(features, model.feature_importances_))
                    print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Stored feature importances for {model_name}")
                    logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: Stored feature importances for {model_name}")
            
            print("üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: All base models trained successfully")
            logger.error("üèãÔ∏è‚Äç‚ôÇÔ∏è BASE: All base models trained successfully")
            return True
            
        except Exception as e:
            print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE ERROR: Error training base models: {e}")
            print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE ERROR: Error training base models: {e}")
            logger.error(f"üèãÔ∏è‚Äç‚ôÇÔ∏è BASE ERROR TRACEBACK:\n{traceback.format_exc()}")
            return False
    
    def _create_feature_subsets(self, all_feature_cols):
        """
        Create feature subsets from all feature columns.
        
        Args:
            all_feature_cols: All available feature columns
            
        Returns:
            dict: Dictionary of feature subsets
        """
        print("üìä SUBSET: _create_feature_subsets called")
        logger.error("üìä SUBSET: _create_feature_subsets called")
        
        # 1. Technical indicators subset
        tech_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'sma', 'ema', 'macd', 'rsi', 'bband', 'stoch', 'adx', 'atr', 'obv', 'cci', 'willr'
        ])]
        
        # 2. Volatility indicators subset
        vol_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'volatility', 'garch', 'atr', 'range', 'std', 'bband', 'vix', 'variance'
        ])]
        
        # 3. Momentum indicators subset
        mom_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'momentum', 'rsi', 'macd', 'roc', 'willr', 'stoch', 'trix', 'ao', 'acceleration'
        ])]
        
        # 4. Price action subset
        price_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'close', 'price', 'open', 'high', 'low', 'range', 'lag', 'change', 'return', 'gap'
        ])]
        
        # 5. Volume indicators subset
        vol_ind_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'volume', 'vwap', 'obv', 'mfi', 'ad', 'cmf', 'vpt', 'pvt'
        ])]
        
        # 6. Sentiment and options subset
        sent_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'sentiment', 'social', 'news', 'option', 'call', 'put', 'unusual', 'headline', 'fear', 'greed'
        ])]
        
        # 7. Market regime subset
        regime_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'regime', 'trend', 'bull', 'bear', 'sideways', 'correction', 'recovery'
        ])]
        
        # 8. Order flow subset
        order_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'flow', 'imbalance', 'delta', 'vwap', 'dark', 'pressure', 'market', 'limit', 'bid', 'ask', 'spread'
        ])]
        
        # 9. Mean reversion subset
        mean_rev_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'zscore', 'mean', 'reversion', 'overextended', 'overbought', 'oversold'
        ])]
        
        # 10. Correlation and relative strength subset
        corr_features = [col for col in all_feature_cols if any(x in col.lower() for x in [
            'correlation', 'relative', 'strength', 'beta', 'alpha', 'sector', 'industry'
        ])]
        
        # Combine into feature subsets
        feature_subsets = {
            'all_features': all_feature_cols,
            'technical': tech_features,
            'volatility': vol_features,
            'momentum': mom_features,
            'price_action': price_features,
            'volume': vol_ind_features,
            'sentiment': sent_features,
            'market_regime': regime_features,
            'order_flow': order_features,
            'mean_reversion': mean_rev_features,
            'correlation': corr_features
        }
        
        # Filter out empty subsets and ensure minimum features
        feature_subsets = {k: v for k, v in feature_subsets.items() if len(v) >= 5}
        
        # Log feature subset counts
        for name, features in feature_subsets.items():
            print(f"üìä SUBSET: {name} has {len(features)} features")
            logger.error(f"üìä SUBSET: {name} has {len(features)} features")
        
        # If we don't have enough specialized features, use random feature subsets
        if len(feature_subsets) < 3:
            print("üìä SUBSET: Not enough specialized feature subsets, using random subsets")
            logger.error("üìä SUBSET: Not enough specialized feature subsets, using random subsets")
            np.random.seed(self.config['random_state'])  # For reproducibility
            n_features = min(50, len(all_feature_cols))
            feature_subsets = {
                'all_features': all_feature_cols,
                'subset1': np.random.choice(all_feature_cols, n_features, replace=False).tolist(),
                'subset2': np.random.choice(all_feature_cols, n_features, replace=False).tolist()
            }
            
        return feature_subsets
    
    def _create_meta_features(self, X_train, X_test):
        """
        Create meta-features by getting predictions from all base models.
        These meta-features will be used to train the meta-learner.
        
        Args:
            X_train: Training features
            X_test: Test features
            
        Returns:
            tuple: (meta_features_train, meta_features_test)
        """
        try:
            print("üß© META: _create_meta_features called")
            logger.error("üß© META: _create_meta_features called")
            
            meta_features_train = pd.DataFrame()
            meta_features_test = pd.DataFrame()
            
            # Get predictions from each model
            for model_name, model in self.models.items():
                print(f"üß© META: Getting predictions from {model_name}...")
                logger.error(f"üß© META: Getting predictions from {model_name}...")
                
                # Get feature subset and scaler for this model
                features = self.feature_subsets[model_name]
                scaler = self.scalers[model_name]
                
                # Scale features
                X_train_scaled = scaler.transform(X_train[features])
                X_test_scaled = scaler.transform(X_test[features])
                
                # Get probability predictions if available
                if hasattr(model, 'predict_proba'):
                    train_preds = model.predict_proba(X_train_scaled)[:, 1]
                    test_preds = model.predict_proba(X_test_scaled)[:, 1]
                else:
                    train_preds = model.predict(X_train_scaled)
                    test_preds = model.predict(X_test_scaled)
                
                # Add predictions as meta-features
                meta_features_train[f"{model_name}_prob"] = train_preds
                meta_features_test[f"{model_name}_prob"] = test_preds
                print(f"üß© META: Added {model_name} predictions to meta-features")
                logger.error(f"üß© META: Added {model_name} predictions to meta-features")
            
            # Add LSTM predictions if available
            if TENSORFLOW_AVAILABLE and self.lstm_model is not None and self.lstm_scaler is not None:
                try:
                    print("üß© META: Getting LSTM predictions...")
                    logger.error("üß© META: Getting LSTM predictions...")
                    # Prepare LSTM input for all samples
                    lstm_X_train = self._prepare_lstm_input(X_train)
                    lstm_X_test = self._prepare_lstm_input(X_test)
                    
                    if lstm_X_train is not None and lstm_X_test is not None:
                        # Get LSTM predictions
                        train_preds = self.lstm_model.predict(lstm_X_train).flatten()
                        test_preds = self.lstm_model.predict(lstm_X_test).flatten()
                        
                        # Validate predictions have the right shape
                        if len(train_preds) == len(X_train) and len(test_preds) == len(X_test):
                            # Add as meta-features
                            meta_features_train['lstm_prob'] = train_preds
                            meta_features_test['lstm_prob'] = test_preds
                            print("üß© META: Added LSTM predictions to meta-features")
                            logger.error("üß© META: Added LSTM predictions to meta-features")
                        else:
                            print(f"üß© META: LSTM predictions shape mismatch: train={len(train_preds)}, expected={len(X_train)}")
                            logger.error(f"üß© META: LSTM predictions shape mismatch: train={len(train_preds)}, expected={len(X_train)}")
                            # Still add lstm_prob with default values
                            meta_features_train['lstm_prob'] = 0.5
                            meta_features_test['lstm_prob'] = 0.5
                            print("üß© META: Added default LSTM values (0.5) to meta-features")
                            logger.error("üß© META: Added default LSTM values (0.5) to meta-features")
                    else:
                        # Add lstm_prob with default values
                        meta_features_train['lstm_prob'] = 0.5
                        meta_features_test['lstm_prob'] = 0.5
                        print("üß© META: Added default LSTM values (0.5) to meta-features")
                        logger.error("üß© META: Added default LSTM values (0.5) to meta-features")
                except Exception as e:
                    print(f"üß© META: Error getting LSTM predictions: {e}")
                    print(f"üß© META: Error traceback:\n{traceback.format_exc()}")
                    logger.error(f"üß© META: Error getting LSTM predictions: {e}")
                    logger.error(f"üß© META: Error traceback:\n{traceback.format_exc()}")
                    # Still add lstm_prob with default values
                    meta_features_train['lstm_prob'] = 0.5
                    meta_features_test['lstm_prob'] = 0.5
                    print("üß© META: Added default LSTM values (0.5) to meta-features due to error")
                    logger.error("üß© META: Added default LSTM values (0.5) to meta-features due to error")
            else:
                # Always add lstm_prob even if LSTM is not available
                meta_features_train['lstm_prob'] = 0.5
                meta_features_test['lstm_prob'] = 0.5
                print("üß© META: Added default LSTM predictions (0.5) to meta-features")
                logger.error("üß© META: Added default LSTM predictions (0.5) to meta-features")
            
            print(f"üß© META: Created meta-features with shape: {meta_features_train.shape}")
            logger.error(f"üß© META: Created meta-features with shape: {meta_features_train.shape}")
            
            # Debug log the metadata features columns
            print(f"üß© META: Meta-features train columns: {meta_features_train.columns.tolist()}")
            logger.error(f"üß© META: Meta-features train columns: {meta_features_train.columns.tolist()}")
            print(f"üß© META: Meta-features test columns: {meta_features_test.columns.tolist()}")
            logger.error(f"üß© META: Meta-features test columns: {meta_features_test.columns.tolist()}")
            
            return meta_features_train, meta_features_test
            
        except Exception as e:
            print(f"üß© META ERROR: Error creating meta-features: {e}")
            print(f"üß© META ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üß© META ERROR: Error creating meta-features: {e}")
            logger.error(f"üß© META ERROR TRACEBACK:\n{traceback.format_exc()}")
            # Return empty dataframes if there's an error
            return pd.DataFrame(), pd.DataFrame()

    def _train_meta_learner(self, meta_features_train, y_train, meta_features_test, y_test):
        """
        Train the meta-learner model using meta-features.
        
        Args:
            meta_features_train: Meta-features for training
            y_train: Training labels
            meta_features_test: Meta-features for testing
            y_test: Test labels
            
        Returns:
            bool: Success or failure
        """
        try:
            print("ü§ñ META-LEARN: _train_meta_learner called")
            logger.error("ü§ñ META-LEARN: _train_meta_learner called")
            
            if meta_features_train.empty or meta_features_test.empty:
                print("ü§ñ META-LEARN: Empty meta-features, cannot train meta-learner")
                logger.error("ü§ñ META-LEARN: Empty meta-features, cannot train meta-learner")
                return False
                
            # First, ensure meta-features have consistent columns across train and test
            all_columns = set(meta_features_train.columns) | set(meta_features_test.columns)
            
            # Add missing columns with zeros
            for col in all_columns:
                if col not in meta_features_train.columns:
                    meta_features_train[col] = 0.5  # Default value
                    print(f"ü§ñ META-LEARN: Added missing column {col} to train set")
                    logger.error(f"ü§ñ META-LEARN: Added missing column {col} to train set")
                if col not in meta_features_test.columns:
                    meta_features_test[col] = 0.5  # Default value
                    print(f"ü§ñ META-LEARN: Added missing column {col} to test set")
                    logger.error(f"ü§ñ META-LEARN: Added missing column {col} to test set")
            
            # Create and fit scaler for meta-features
            self.meta_scaler = StandardScaler()
            meta_train_scaled = self.meta_scaler.fit_transform(meta_features_train)
            meta_test_scaled = self.meta_scaler.transform(meta_features_test)
            print("ü§ñ META-LEARN: Applied StandardScaler to meta-features")
            logger.error("ü§ñ META-LEARN: Applied StandardScaler to meta-features")
            
            # Verify we have 'lstm_prob' in our features
            if 'lstm_prob' in meta_features_train.columns:
                print("ü§ñ META-LEARN: 'lstm_prob' is present in meta-features")
                logger.error("ü§ñ META-LEARN: 'lstm_prob' is present in meta-features")
            else:
                print("ü§ñ META-LEARN: WARNING - 'lstm_prob' is MISSING from meta-features!")
                logger.error("ü§ñ META-LEARN: WARNING - 'lstm_prob' is MISSING from meta-features!")
            
            # Select meta-learner based on configuration
            meta_learner_type = self.config.get('meta_learner_type', 'xgboost')
            print(f"ü§ñ META-LEARN: Using {meta_learner_type} for meta-learner")
            logger.error(f"ü§ñ META-LEARN: Using {meta_learner_type} for meta-learner")
            
            if meta_learner_type == 'xgboost':
                # XGBoost meta-learner
                self.meta_model = XGBClassifier(
                    n_estimators=200,
                    learning_rate=0.05,
                    max_depth=4,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    use_label_encoder=False,
                    eval_metric='logloss',
                    random_state=self.config['random_state']
                )
            elif meta_learner_type == 'lightgbm':
                # LightGBM meta-learner
                self.meta_model = LGBMClassifier(
                    n_estimators=200,
                    learning_rate=0.05,
                    num_leaves=32,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    random_state=self.config['random_state']
                )
            elif meta_learner_type == 'randomforest':
                # Random Forest meta-learner
                self.meta_model = RandomForestClassifier(
                    n_estimators=200,
                    max_depth=5,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=self.config['random_state']
                )
            elif meta_learner_type == 'voting':
                # Voting meta-learner (combines multiple meta-models)
                meta_models = [
                    ('xgb', XGBClassifier(
                        n_estimators=100, learning_rate=0.05, max_depth=3,
                        use_label_encoder=False, eval_metric='logloss',
                        random_state=self.config['random_state']
                    )),
                    ('lgb', LGBMClassifier(
                        n_estimators=100, learning_rate=0.05, num_leaves=32,
                        random_state=self.config['random_state']
                    )),
                    ('rf', RandomForestClassifier(
                        n_estimators=100, max_depth=5,
                        random_state=self.config['random_state']
                    ))
                ]
                
                self.meta_model = VotingClassifier(
                    estimators=meta_models,
                    voting='soft',
                    weights=[2, 1, 1]
                )
            else:
                # Default to XGBoost
                self.meta_model = XGBClassifier(
                    n_estimators=100,
                    learning_rate=0.05,
                    max_depth=3,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    use_label_encoder=False,
                    eval_metric='logloss',
                    random_state=self.config['random_state']
                )
            
            # Train meta-model with early stopping if supported
            if hasattr(self.meta_model, 'fit') and hasattr(self.meta_model, 'predict_proba'):
                # For models supporting early stopping
                if meta_learner_type in ['xgboost', 'lightgbm']:
                    # Split for validation
                    valid_size = 0.2
                    valid_samples = int(meta_train_scaled.shape[0] * valid_size)
                    
                    X_train_meta = meta_train_scaled[:-valid_samples]
                    X_valid_meta = meta_train_scaled[-valid_samples:]
                    y_train_meta = y_train.iloc[:-valid_samples]
                    y_valid_meta = y_train.iloc[-valid_samples:]
                    
                    eval_set = [(X_valid_meta, y_valid_meta)]
                    
                    print(f"ü§ñ META-LEARN: Training {meta_learner_type} with early stopping")
                    logger.error(f"ü§ñ META-LEARN: Training {meta_learner_type} with early stopping")
                    if meta_learner_type == 'xgboost':
                        self.meta_model.fit(
                            X_train_meta, y_train_meta,
                            eval_set=eval_set,
                            early_stopping_rounds=20,
                            verbose=False
                        )
                    elif meta_learner_type == 'lightgbm':
                        self.meta_model.fit(
                            X_train_meta, y_train_meta,
                            eval_set=eval_set,
                            early_stopping_rounds=20,
                            verbose=False
                        )
                else:
                    # Standard fit for other models
                    print(f"ü§ñ META-LEARN: Training {meta_learner_type} with standard fit")
                    logger.error(f"ü§ñ META-LEARN: Training {meta_learner_type} with standard fit")
                    self.meta_model.fit(meta_train_scaled, y_train)
            else:
                # Standard fit
                print("ü§ñ META-LEARN: Training meta-model with standard fit")
                logger.error("ü§ñ META-LEARN: Training meta-model with standard fit")
                self.meta_model.fit(meta_train_scaled, y_train)
            
            # Store feature names from training
            self.meta_feature_names = meta_features_train.columns.tolist()
            print(f"ü§ñ META-LEARN: Stored meta_feature_names: {self.meta_feature_names}")
            logger.error(f"ü§ñ META-LEARN: Stored meta_feature_names: {self.meta_feature_names}")
            
            # If the model has feature_names_in_, try to check if it matches our stored feature names
            if hasattr(self.meta_model, 'feature_names_in_'):
                print(f"ü§ñ META-LEARN: Model's feature_names_in_: {self.meta_model.feature_names_in_}")
                logger.error(f"ü§ñ META-LEARN: Model's feature_names_in_: {self.meta_model.feature_names_in_}")
                
                # Check for mismatch
                if len(self.meta_feature_names) != len(self.meta_model.feature_names_in_):
                    print("ü§ñ META-LEARN: WARNING - Length mismatch in feature names!")
                    logger.error("ü§ñ META-LEARN: WARNING - Length mismatch in feature names!")
            
            # Evaluate meta-model
            y_pred = self.meta_model.predict(meta_test_scaled)
            
            metrics = {
                'accuracy': accuracy_score(y_test, y_pred),
                'f1_score': f1_score(y_test, y_pred),
                'f2_score': fbeta_score(y_test, y_pred, beta=2),
                'precision': precision_score(y_test, y_pred),
                'recall': recall_score(y_test, y_pred),
            }
            
            # Add AUC if available
            if hasattr(self.meta_model, 'predict_proba'):
                try:
                    proba = self.meta_model.predict_proba(meta_test_scaled)[:, 1]
                    metrics['auc'] = roc_auc_score(y_test, proba)
                except:
                    metrics['auc'] = 0.5
            else:
                metrics['auc'] = 0.5
            
            # Add confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            metrics['confusion_matrix'] = cm.tolist()
            
            self.model_metrics['meta_learner'] = metrics
            
            print(f"ü§ñ META-LEARN: Meta-learner trained with f2_score: {metrics['f2_score']:.4f}, AUC: {metrics.get('auc', 0):.4f}")
            logger.error(f"ü§ñ META-LEARN: Meta-learner trained with f2_score: {metrics['f2_score']:.4f}, AUC: {metrics.get('auc', 0):.4f}")
            
            return True
            
        except Exception as e:
            print(f"ü§ñ META-LEARN ERROR: Error training meta-learner: {e}")
            print(f"ü§ñ META-LEARN ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"ü§ñ META-LEARN ERROR: Error training meta-learner: {e}")
            logger.error(f"ü§ñ META-LEARN ERROR TRACEBACK:\n{traceback.format_exc()}")
            return False

    def _evaluate_ensemble(self, X_test, y_test):
        """
        Evaluate the complete ensemble on test data.
        
        Args:
            X_test: Test features
            y_test: Test labels
            
        Returns:
            dict: Evaluation metrics
        """
        try:
            print("üìä EVAL: _evaluate_ensemble called")
            logger.error("üìä EVAL: _evaluate_ensemble called")
            
            # Initialize arrays for predictions
            y_preds = []
            probas = []
            
            # Get ensemble predictions for each sample
            for i in range(len(X_test)):
                # Get prediction for this sample
                sample = X_test.iloc[[i]] if hasattr(X_test, 'iloc') else X_test[i:i+1]
                prob, pred = self.predict_with_ensemble(sample)
                
                y_preds.append(pred)
                probas.append(prob)
                
                if i % 10 == 0:
                    print(f"üìä EVAL: Evaluated {i}/{len(X_test)} samples")
                    logger.error(f"üìä EVAL: Evaluated {i}/{len(X_test)} samples")
            
            # Convert to numpy arrays
            y_preds = np.array(y_preds)
            probas = np.array(probas)
            
            # Calculate metrics
            metrics = {
                'accuracy': accuracy_score(y_test, y_preds),
                'f1_score': f1_score(y_test, y_preds),
                'f2_score': fbeta_score(y_test, y_preds, beta=2),
                'precision': precision_score(y_test, y_preds),
                'recall': recall_score(y_test, y_preds),
                'auc': roc_auc_score(y_test, probas)
            }
            
            # Add confusion matrix
            cm = confusion_matrix(y_test, y_preds)
            metrics['confusion_matrix'] = cm.tolist()
            
            # Store in model_metrics
            self.model_metrics['ensemble'] = metrics
            
            print(f"üìä EVAL: Ensemble evaluation: {metrics}")
            logger.error(f"üìä EVAL: Ensemble evaluation: {metrics}")
            
            return metrics
        
        except Exception as e:
            print(f"üìä EVAL ERROR: Error evaluating ensemble: {e}")
            print(f"üìä EVAL ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üìä EVAL ERROR: Error evaluating ensemble: {e}")
            logger.error(f"üìä EVAL ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'error': str(e)}

    def _build_lstm_model(self, input_shape, hyperparams=None):
        """
        Build an advanced LSTM model architecture with attention.
        
        Args:
            input_shape: Input shape for the model (sequence_length, n_features)
            hyperparams: Optional hyperparameters for the model
            
        Returns:
            keras.Model: Compiled LSTM model
        """
        if not TENSORFLOW_AVAILABLE:
            print("üß† LSTM BUILD: TensorFlow not available, cannot build LSTM model")
            logger.error("üß† LSTM BUILD: TensorFlow not available, cannot build LSTM model")
            return None
            
        try:
            print("üß† LSTM BUILD: Building LSTM model...")
            logger.error("üß† LSTM BUILD: Building LSTM model...")
            
            # Default hyperparameters
            hp = {
                'lstm_units1': 64,
                'lstm_units2': 32,
                'dense_units': 16,
                'dropout_rate': 0.3,
                'learning_rate': 0.001,
                'use_attention': self.config.get('use_attention', True),
                'use_bidirectional': True
            }
            
            # Update with provided hyperparameters if any
            if hyperparams:
                hp.update(hyperparams)
            
            print(f"üß† LSTM BUILD: Hyperparameters: {hp}")
            logger.error(f"üß† LSTM BUILD: Hyperparameters: {hp}")
            
            # Create model
            model = Sequential()
            
            # First LSTM layer
            if hp['use_bidirectional']:
                # Bidirectional LSTM with return sequences for attention
                model.add(Bidirectional(
                    LSTM(units=hp['lstm_units1'], 
                         return_sequences=True),
                    input_shape=input_shape
                ))
                print("üß† LSTM BUILD: Added Bidirectional LSTM layer")
                logger.error("üß† LSTM BUILD: Added Bidirectional LSTM layer")
            else:
                # Standard LSTM
                model.add(LSTM(
                    units=hp['lstm_units1'],
                    return_sequences=True,
                    input_shape=input_shape
                ))
                print("üß† LSTM BUILD: Added standard LSTM layer")
                logger.error("üß† LSTM BUILD: Added standard LSTM layer")
            
            # Add normalization and dropout
            model.add(LayerNormalization())  # Layer normalization for better stability
            model.add(Dropout(hp['dropout_rate']))
            print("üß† LSTM BUILD: Added normalization and dropout")
            logger.error("üß† LSTM BUILD: Added normalization and dropout")
            
            # Add attention mechanism if requested
            if hp['use_attention']:
                print("üß† LSTM BUILD: Adding attention mechanism")
                logger.error("üß† LSTM BUILD: Adding attention mechanism")
                # Implement a simple self-attention mechanism
                # Replace sequential model with functional API for attention
                inputs = keras.Input(shape=input_shape)
                x = Bidirectional(LSTM(units=hp['lstm_units1'], return_sequences=True))(inputs)
                x = LayerNormalization()(x)
                x = Dropout(hp['dropout_rate'])(x)
                
                # Apply attention
                attention = Conv1D(1, kernel_size=1)(x)
                attention = keras.activations.tanh(attention)
                attention_weights = keras.layers.Softmax(axis=1)(attention)
                context = keras.layers.Multiply()([x, attention_weights])
                context = keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(context)
                
                # Continue with dense layers
                x = Dense(hp['dense_units'], activation='relu')(context)
                x = Dropout(hp['dropout_rate'])(x)
                outputs = Dense(1, activation='sigmoid')(x)
                
                # Create model
                model = keras.Model(inputs=inputs, outputs=outputs)
                print("üß† LSTM BUILD: Built LSTM model with attention")
                logger.error("üß† LSTM BUILD: Built LSTM model with attention")
            else:
                # Standard setup without attention
                model.add(LSTM(units=hp['lstm_units2'], return_sequences=False))
                model.add(BatchNormalization())
                model.add(Dropout(hp['dropout_rate']))
                model.add(Dense(hp['dense_units'], activation='relu'))
                model.add(Dense(1, activation='sigmoid'))
                print("üß† LSTM BUILD: Built standard LSTM model")
                logger.error("üß† LSTM BUILD: Built standard LSTM model")
            
            # Compile model
            model.compile(
                optimizer=Adam(learning_rate=hp['learning_rate']),
                loss='binary_crossentropy',
                metrics=['accuracy']
            )
            
            print("üß† LSTM BUILD: LSTM model compiled successfully")
            logger.error("üß† LSTM BUILD: LSTM model compiled successfully")
            return model
            
        except Exception as e:
            print(f"üß† LSTM BUILD ERROR: Error building LSTM model: {e}")
            print(f"üß† LSTM BUILD ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üß† LSTM BUILD ERROR: Error building LSTM model: {e}")
            logger.error(f"üß† LSTM BUILD ERROR TRACEBACK:\n{traceback.format_exc()}")
            return None
    def _train_lstm_model(self, all_data, all_feature_cols, sequence_length=None):
        """
        Train an LSTM model for sequence prediction.
        
        Args:
            all_data: Full dataset
            all_feature_cols: All available feature columns
            sequence_length: Length of sequences to use (default: use self.lstm_sequence_length)
            
        Returns:
            bool: Success or failure
        """
        if not TENSORFLOW_AVAILABLE:
            print("üß† LSTM TRAIN: TensorFlow not available, skipping LSTM training")
            logger.error("üß† LSTM TRAIN: TensorFlow not available, skipping LSTM training")
            return False
        
        try:
            print("üß† LSTM TRAIN: Training LSTM model...")
            logger.error("üß† LSTM TRAIN: Training LSTM model...")
            
            # Set sequence length if provided
            if sequence_length is not None:
                self.lstm_sequence_length = sequence_length
                print(f"üß† LSTM TRAIN: Using sequence length: {self.lstm_sequence_length}")
                logger.error(f"üß† LSTM TRAIN: Using sequence length: {self.lstm_sequence_length}")
            
            # In production, we'd use more robust feature selection for LSTM
            # Here we're simplifying by using a subset of important features
            if self.feature_importance and self.config.get('use_feature_selection', True):
                # Get top 20 features by importance if available
                top_features = self._get_top_features(20)
                if top_features:
                    lstm_feature_cols = [f for f in top_features if f in all_feature_cols]
                    if len(lstm_feature_cols) >= 5:
                        print(f"üß† LSTM TRAIN: Using {len(lstm_feature_cols)} top features for LSTM")
                        logger.error(f"üß† LSTM TRAIN: Using {len(lstm_feature_cols)} top features for LSTM")
                    else:
                        lstm_feature_cols = all_feature_cols
                        print(f"üß† LSTM TRAIN: Not enough top features, using all {len(all_feature_cols)} features")
                        logger.error(f"üß† LSTM TRAIN: Not enough top features, using all {len(all_feature_cols)} features")
                else:
                    lstm_feature_cols = all_feature_cols
                    print(f"üß† LSTM TRAIN: No feature importance available, using all {len(all_feature_cols)} features")
                    logger.error(f"üß† LSTM TRAIN: No feature importance available, using all {len(all_feature_cols)} features")
            else:
                lstm_feature_cols = all_feature_cols
                print(f"üß† LSTM TRAIN: Using all {len(all_feature_cols)} features")
                logger.error(f"üß† LSTM TRAIN: Using all {len(all_feature_cols)} features")
            
            # Store LSTM feature columns
            self.lstm_feature_cols = lstm_feature_cols
            
            # Prepare data for LSTM
            print("üß† LSTM TRAIN: Preparing data for LSTM...")
            logger.error("üß† LSTM TRAIN: Preparing data for LSTM...")
            data_subset = all_data[self.lstm_feature_cols + ['Label']].copy()
            
            # Handle missing values if any
            data_subset = data_subset.fillna(method='ffill')
            data_subset = data_subset.fillna(method='bfill')
            data_subset = data_subset.fillna(0)  # Fill any remaining NaNs with 0
            
            # Create a robust scaler for LSTM features
            self.lstm_scaler = StandardScaler()
            data_subset[self.lstm_feature_cols] = self.lstm_scaler.fit_transform(data_subset[self.lstm_feature_cols])
            print("üß† LSTM TRAIN: Scaled features with StandardScaler")
            logger.error("üß† LSTM TRAIN: Scaled features with StandardScaler")
            
            # Create sequences
            print("üß† LSTM TRAIN: Creating input sequences...")
            logger.error("üß† LSTM TRAIN: Creating input sequences...")
            X_sequences, y = self._prepare_sequence_data(data_subset, self.lstm_feature_cols)
            
            if X_sequences is None or y is None or len(X_sequences) == 0:
                print("üß† LSTM TRAIN: Could not prepare sequence data for LSTM")
                logger.error("üß† LSTM TRAIN: Could not prepare sequence data for LSTM")
                return False
                
            # Split into train, validation, and test
            train_size = int(len(X_sequences) * 0.7)
            val_size = int(len(X_sequences) * 0.15)
            
            X_train = X_sequences[:train_size]
            y_train = y[:train_size]
            
            X_val = X_sequences[train_size:train_size + val_size]
            y_val = y[train_size:train_size + val_size]
            
            X_test = X_sequences[train_size + val_size:]
            y_test = y[train_size + val_size:]
            
            print(f"üß† LSTM TRAIN: Split data - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
            logger.error(f"üß† LSTM TRAIN: Split data - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
            
            # Build and compile LSTM model
            print("üß† LSTM TRAIN: Building and compiling LSTM model...")
            logger.error("üß† LSTM TRAIN: Building and compiling LSTM model...")
            input_shape = (self.lstm_sequence_length, X_train.shape[2])
            self.lstm_model = self._build_lstm_model(input_shape)
            
            if self.lstm_model is None:
                print("üß† LSTM TRAIN: Failed to build LSTM model")
                logger.error("üß† LSTM TRAIN: Failed to build LSTM model")
                return False
            
            # Create callbacks for LSTM training
            print("üß† LSTM TRAIN: Setting up training callbacks...")
            logger.error("üß† LSTM TRAIN: Setting up training callbacks...")
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=self.config.get('early_stopping_patience', 10),
                    restore_best_weights=True
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=5,
                    min_lr=0.0001
                )
            ]
            
            # Add model checkpoint if directory exists
            checkpoint_dir = os.path.join(self.model_dir, 'checkpoints')
            if not os.path.exists(checkpoint_dir):
                os.makedirs(checkpoint_dir)
                
            checkpoint_path = os.path.join(checkpoint_dir, 'lstm_checkpoint.keras')
            callbacks.append(
                ModelCheckpoint(
                    checkpoint_path,
                    monitor='val_loss',
                    save_best_only=True
                )
            )
            
            # Train model
            print("üß† LSTM TRAIN: Starting LSTM training...")
            logger.error("üß† LSTM TRAIN: Starting LSTM training...")
            history = self.lstm_model.fit(
                X_train, y_train,
                epochs=self.config.get('lstm_epochs', 50),
                batch_size=self.config.get('lstm_batch_size', 32),
                validation_data=(X_val, y_val),
                callbacks=callbacks,
                verbose=1
            )
            
            # Record trained epochs
            self.model_epochs['lstm'] = len(history.history['loss'])
            print(f"üß† LSTM TRAIN: LSTM model trained for {self.model_epochs['lstm']} epochs")
            logger.error(f"üß† LSTM TRAIN: LSTM model trained for {self.model_epochs['lstm']} epochs")
            
            # Evaluate model
            print("üß† LSTM TRAIN: Evaluating LSTM model...")
            logger.error("üß† LSTM TRAIN: Evaluating LSTM model...")
            results = self.lstm_model.evaluate(X_test, y_test)
            
            # Make predictions for test set
            y_pred_proba = self.lstm_model.predict(X_test).flatten()
            y_pred = (y_pred_proba > 0.5).astype(int)
            
            # Calculate detailed metrics
            metrics = {
                'loss': float(results[0]),
                'accuracy': float(results[1]),
                'f1_score': float(f1_score(y_test, y_pred)),
                'f2_score': float(fbeta_score(y_test, y_pred, beta=2)),
                'precision': float(precision_score(y_test, y_pred)),
                'recall': float(recall_score(y_test, y_pred)),
                'auc': float(roc_auc_score(y_test, y_pred_proba))
            }
            
            # Add confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            metrics['confusion_matrix'] = cm.tolist()
            
            # Store metrics
            self.model_metrics['lstm'] = metrics
            
            print(f"üß† LSTM TRAIN: LSTM model trained with accuracy: {metrics['accuracy']:.4f}, AUC: {metrics['auc']:.4f}")
            logger.error(f"üß† LSTM TRAIN: LSTM model trained with accuracy: {metrics['accuracy']:.4f}, AUC: {metrics['auc']:.4f}")
            
            return True
            
        except Exception as e:
            print(f"üß† LSTM TRAIN ERROR: Error training LSTM model: {e}")
            print(f"üß† LSTM TRAIN ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üß† LSTM TRAIN ERROR: Error training LSTM model: {e}")
            logger.error(f"üß† LSTM TRAIN ERROR TRACEBACK:\n{traceback.format_exc()}")
            return False
    
    def _prepare_sequence_data(self, df, feature_cols, target_col='Label', drop_y_na=True):
        """
        Prepare sequence data for LSTM model.
        
        Args:
            df: DataFrame with features and target
            feature_cols: Feature columns to use
            target_col: Target column name
            drop_y_na: Whether to drop rows with NA in target
            
        Returns:
            tuple: (X_sequences, y_target) or (None, None) if error
        """
        try:
            print("üîÑ SEQ: _prepare_sequence_data called")
            logger.error("üîÑ SEQ: _prepare_sequence_data called")
            
            # Check if DataFrame is not empty
            if df.empty:
                print("üîÑ SEQ: Empty DataFrame provided for sequence preparation")
                logger.error("üîÑ SEQ: Empty DataFrame provided for sequence preparation")
                return None, None
                
            # Must be sorted by time (assume it's already sorted)
            if 'Timestamp' in df.columns:
                df = df.sort_values('Timestamp').reset_index(drop=True)
                print("üîÑ SEQ: Sorted data by Timestamp")
                logger.error("üîÑ SEQ: Sorted data by Timestamp")
            elif 'Date' in df.columns:
                df = df.sort_values('Date').reset_index(drop=True)
                print("üîÑ SEQ: Sorted data by Date")
                logger.error("üîÑ SEQ: Sorted data by Date")
            
            # Handle missing values in target if needed
            if drop_y_na and df[target_col].isna().any():
                df = df.dropna(subset=[target_col])
                print(f"üîÑ SEQ: Dropped rows with NaN in {target_col}")
                logger.error(f"üîÑ SEQ: Dropped rows with NaN in {target_col}")
            
            # Get features and target
            X = df[feature_cols].values
            y = df[target_col].values
            
            # Create sequences
            X_sequences = []
            y_target = []
            
            # Check if we have enough data for at least one sequence
            if len(X) <= self.lstm_sequence_length:
                print(f"üîÑ SEQ: Not enough data points ({len(X)}) for sequence length {self.lstm_sequence_length}")
                logger.error(f"üîÑ SEQ: Not enough data points ({len(X)}) for sequence length {self.lstm_sequence_length}")
                return None, None
            
            for i in range(len(X) - self.lstm_sequence_length):
                X_sequences.append(X[i:(i + self.lstm_sequence_length)])
                y_target.append(y[i + self.lstm_sequence_length])
            
            # Convert to numpy arrays
            X_sequences = np.array(X_sequences)
            y_target = np.array(y_target)
            
            print(f"üîÑ SEQ: Created {len(X_sequences)} sequences with shape {X_sequences.shape}")
            logger.error(f"üîÑ SEQ: Created {len(X_sequences)} sequences with shape {X_sequences.shape}")
            
            return X_sequences, y_target
            
        except Exception as e:
            print(f"üîÑ SEQ ERROR: Error preparing sequence data: {e}")
            print(f"üîÑ SEQ ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üîÑ SEQ ERROR: Error preparing sequence data: {e}")
            logger.error(f"üîÑ SEQ ERROR TRACEBACK:\n{traceback.format_exc()}")
            return None, None
    
    def _prepare_lstm_input(self, X):
        """
        Prepare input data for LSTM prediction.
        
        Args:
            X: DataFrame with features
            
        Returns:
            numpy.ndarray: Prepared LSTM input or None if preparation fails
        """
        if not TENSORFLOW_AVAILABLE or self.lstm_model is None or self.lstm_scaler is None:
            print("üß† LSTM INPUT: TensorFlow/model/scaler not available")
            logger.error("üß† LSTM INPUT: TensorFlow/model/scaler not available")
            return None
            
        try:
            print("üß† LSTM INPUT: _prepare_lstm_input called")
            logger.error("üß† LSTM INPUT: _prepare_lstm_input called")
            
            # Check if we have stored LSTM feature columns
            lstm_feature_cols = self.lstm_feature_cols
            
            # If not, fall back to all feature columns
            if not lstm_feature_cols:
                lstm_feature_cols = self.all_feature_cols
                print("üß† LSTM INPUT: Using all_feature_cols as fallback")
                logger.error("üß† LSTM INPUT: Using all_feature_cols as fallback")
            
            # Check which columns are available
            available_cols = [col for col in lstm_feature_cols if col in X.columns]
            
            print(f"üß† LSTM INPUT: {len(available_cols)}/{len(lstm_feature_cols)} LSTM features available")
            logger.error(f"üß† LSTM INPUT: {len(available_cols)}/{len(lstm_feature_cols)} LSTM features available")
            
            # If too few columns are available, return None
            if len(available_cols) < 0.5 * len(lstm_feature_cols):
                print(f"üß† LSTM INPUT: Too few LSTM features available: {len(available_cols)}/{len(lstm_feature_cols)}")
                logger.error(f"üß† LSTM INPUT: Too few LSTM features available: {len(available_cols)}/{len(lstm_feature_cols)}")
                return None
                
            # Use available columns
            X_subset = X[available_cols].copy()
            
            # Handle missing values
            X_subset = X_subset.fillna(method='ffill')
            X_subset = X_subset.fillna(method='bfill')
            X_subset = X_subset.fillna(0)  # Fill any remaining NaNs with 0
            
            print("üß† LSTM INPUT: Handled missing values in input data")
            logger.error("üß† LSTM INPUT: Handled missing values in input data")
            
            # Scale features
            X_scaled = self.lstm_scaler.transform(X_subset)
            print("üß† LSTM INPUT: Scaled input features")
            logger.error("üß† LSTM INPUT: Scaled input features")
            
            # For single sample prediction
            if len(X_subset) == 1:
                print("üß† LSTM INPUT: Creating sequence for single sample")
                logger.error("üß† LSTM INPUT: Creating sequence for single sample")
                # Create a sequence with repeated values
                sequence = np.repeat(X_scaled, self.lstm_sequence_length, axis=0)
                sequence = sequence.reshape(1, self.lstm_sequence_length, -1)
                print(f"üß† LSTM INPUT: Created single sample sequence with shape {sequence.shape}")
                logger.error(f"üß† LSTM INPUT: Created single sample sequence with shape {sequence.shape}")
                return sequence
                
            # For multiple samples (during training/evaluation)
            else:
                print("üß† LSTM INPUT: Creating sequences for multiple samples")
                logger.error("üß† LSTM INPUT: Creating sequences for multiple samples")
                # Create sequences from the dataset
                sequences = []
                for i in range(len(X_scaled) - self.lstm_sequence_length + 1):
                    seq = X_scaled[i:i+self.lstm_sequence_length]
                    sequences.append(seq)
                
                # If we need more sequences to match X length
                if len(sequences) < len(X_scaled):
                    padding_needed = len(X_scaled) - len(sequences)
                    print(f"üß† LSTM INPUT: Adding {padding_needed} padding sequences")
                    logger.error(f"üß† LSTM INPUT: Adding {padding_needed} padding sequences")
                    
                    # Repeat the first sequence with padding
                    for i in range(padding_needed):
                        # Create padding by repeating the first row
                        padding = np.repeat(X_scaled[0:1], self.lstm_sequence_length - 1, axis=0)
                        # Add the first i+1 actual values
                        seq = np.vstack([padding[:-(i+1)], X_scaled[:(i+1)]])
                        sequences.insert(0, seq)
                
                sequences_array = np.array(sequences)
                print(f"üß† LSTM INPUT: Created {len(sequences)} sequences with shape {sequences_array.shape}")
                logger.error(f"üß† LSTM INPUT: Created {len(sequences)} sequences with shape {sequences_array.shape}")
                return sequences_array
        
        except Exception as e:
            print(f"üß† LSTM INPUT ERROR: Error preparing LSTM input: {e}")
            print(f"üß† LSTM INPUT ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üß† LSTM INPUT ERROR: Error preparing LSTM input: {e}")
            logger.error(f"üß† LSTM INPUT ERROR TRACEBACK:\n{traceback.format_exc()}")
            return None

    def predict_with_ensemble(self, X):
        """
        Make a prediction using the full ensemble model.
        
        Args:
            X: DataFrame with features
            
        Returns:
            tuple: (probability, prediction)
        """
        try:
            # Debug logging
            print("üîÆ PREDICT: predict_with_ensemble called")
            logger.error("üîÆ PREDICT: predict_with_ensemble called")
            
            print(f"üîÆ PREDICT: Input shape: {X.shape if hasattr(X, 'shape') else 'unknown'}")
            logger.error(f"üîÆ PREDICT: Input shape: {X.shape if hasattr(X, 'shape') else 'unknown'}")
            
            print(f"üîÆ PREDICT: Meta-model exists: {self.meta_model is not None}")
            logger.error(f"üîÆ PREDICT: Meta-model exists: {self.meta_model is not None}")
            
            print(f"üîÆ PREDICT: Meta-scaler exists: {self.meta_scaler is not None}")
            logger.error(f"üîÆ PREDICT: Meta-scaler exists: {self.meta_scaler is not None}")
            
            # Sanity check on input
            if X is None or len(X) == 0:
                print("üîÆ PREDICT: Empty input for prediction")
                logger.error("üîÆ PREDICT: Empty input for prediction")
                return 0.5, 0
            
            # Handle missing features
            if not self.all_feature_cols:
                print("üîÆ PREDICT: No feature columns defined")
                logger.error("üîÆ PREDICT: No feature columns defined")
                return 0.5, 0
                
            # Make sure X has all required columns with appropriate handling for missing columns
            missing_cols = [col for col in self.all_feature_cols if col not in X.columns]
            if missing_cols:
                print(f"üîÆ PREDICT: Missing {len(missing_cols)}/{len(self.all_feature_cols)} feature columns. Using defaults.")
                logger.error(f"üîÆ PREDICT: Missing {len(missing_cols)}/{len(self.all_feature_cols)} feature columns. Using defaults.")
                
                # Add missing columns with default values
                for col in missing_cols:
                    X[col] = 0.0
                    
                print(f"üîÆ PREDICT: Added missing columns with default values")
                logger.error(f"üîÆ PREDICT: Added missing columns with default values")
            
            # Initialize dictionary to store model predictions
            model_predictions = {}
            
            # Get predictions from each model
            for model_name, model in self.models.items():
                print(f"üîÆ PREDICT: Getting prediction from {model_name}")
                logger.error(f"üîÆ PREDICT: Getting prediction from {model_name}")
                
                # Get feature subset and scaler for this model
                features = self.feature_subsets.get(model_name, [])
                scaler = self.scalers.get(model_name)
                
                if not features or scaler is None:
                    print(f"üîÆ PREDICT: Missing features or scaler for {model_name}")
                    logger.error(f"üîÆ PREDICT: Missing features or scaler for {model_name}")
                    continue
                
                # Check if all required features exist
                missing_features = [f for f in features if f not in X.columns]
                if missing_features:
                    print(f"üîÆ PREDICT: Missing features for {model_name}: {missing_features}")
                    logger.error(f"üîÆ PREDICT: Missing features for {model_name}: {missing_features}")
                    # Add missing features with zeros
                    for f in missing_features:
                        X[f] = 0.0
                    print(f"üîÆ PREDICT: Added missing features with default values for {model_name}")
                    logger.error(f"üîÆ PREDICT: Added missing features with default values for {model_name}")
                
            try:
                    # Scale features
                    X_scaled = scaler.transform(X[features])
                    
                    # Get probability prediction
                    if hasattr(model, 'predict_proba'):
                        prob = model.predict_proba(X_scaled)[:, 1][0]  # Get probability of positive class
                        print(f"üîÆ PREDICT: {model_name} probability: {prob:.4f}")
                        logger.error(f"üîÆ PREDICT: {model_name} probability: {prob:.4f}")
                    else:
                        prob = float(model.predict(X_scaled)[0])  # Convert prediction to probability
                        print(f"üîÆ PREDICT: {model_name} prediction: {prob}")
                        logger.error(f"üîÆ PREDICT: {model_name} prediction: {prob}")
                    
                    model_predictions[f"{model_name}_prob"] = prob
            except Exception as e:
                    print(f"üîÆ PREDICT: Error getting prediction from {model_name}: {e}")
                    print(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
                    logger.error(f"üîÆ PREDICT: Error getting prediction from {model_name}: {e}")
                    logger.error(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
                    model_predictions[f"{model_name}_prob"] = 0.5  # Default value
                    print(f"üîÆ PREDICT: Using default value 0.5 for {model_name}")
                    logger.error(f"üîÆ PREDICT: Using default value 0.5 for {model_name}")
            
            # Add LSTM prediction if available
            if TENSORFLOW_AVAILABLE and self.lstm_model is not None:
                print("üîÆ PREDICT: Getting LSTM prediction")
                logger.error("üîÆ PREDICT: Getting LSTM prediction")
                try:
                    lstm_X = self._prepare_lstm_input(X)
                    if lstm_X is not None:
                        lstm_prob = float(self.lstm_model.predict(lstm_X).flatten()[0])
                        model_predictions['lstm_prob'] = lstm_prob
                        print(f"üîÆ PREDICT: LSTM probability: {lstm_prob:.4f}")
                        logger.error(f"üîÆ PREDICT: LSTM probability: {lstm_prob:.4f}")
                    else:
                        model_predictions['lstm_prob'] = 0.5  # Default value
                        print("üîÆ PREDICT: Could not prepare LSTM input, using default value 0.5")
                        logger.error("üîÆ PREDICT: Could not prepare LSTM input, using default value 0.5")
                except Exception as e:
                    print(f"üîÆ PREDICT: Error getting LSTM prediction: {e}")
                    print(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
                    logger.error(f"üîÆ PREDICT: Error getting LSTM prediction: {e}")
                    logger.error(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
                    model_predictions['lstm_prob'] = 0.5  # Default value
                    print("üîÆ PREDICT: Using default value 0.5 for LSTM")
                    logger.error("üîÆ PREDICT: Using default value 0.5 for LSTM")
            else:
                model_predictions['lstm_prob'] = 0.5  # Default value
                print("üîÆ PREDICT: No LSTM model available, using default value 0.5")
                logger.error("üîÆ PREDICT: No LSTM model available, using default value 0.5")
            
            # Debug logging of meta features
            print(f"üîÆ PREDICT: Created meta features with keys: {list(model_predictions.keys())}")
            logger.error(f"üîÆ PREDICT: Created meta features with keys: {list(model_predictions.keys())}")
            
            # If no models could make predictions, return default values
            if not model_predictions:
                print("üîÆ PREDICT: No valid predictions from any model")
                logger.error("üîÆ PREDICT: No valid predictions from any model")
                return 0.5, 0
            
            # If meta-model is available, use it to combine predictions
            if self.meta_model is not None and self.meta_scaler is not None and self.meta_feature_names:
                try:
                    print("üîÆ PREDICT: Using meta-model for final prediction")
                    logger.error("üîÆ PREDICT: Using meta-model for final prediction")
                    
                    # Create meta-features DataFrame with expected columns
                    meta_df = pd.DataFrame([model_predictions])
                    
                    # Debug log expected meta feature names
                    print(f"üîÆ PREDICT: Expected meta feature names: {self.meta_feature_names}")
                    logger.error(f"üîÆ PREDICT: Expected meta feature names: {self.meta_feature_names}")
                    print(f"üîÆ PREDICT: Meta DataFrame columns: {meta_df.columns.tolist()}")
                    logger.error(f"üîÆ PREDICT: Meta DataFrame columns: {meta_df.columns.tolist()}")
                    
                    # Ensure all expected features are present
                    for col in self.meta_feature_names:
                        if col not in meta_df.columns:
                            meta_df[col] = 0.5  # Add missing columns with default value
                            print(f"üîÆ PREDICT: Added missing meta feature '{col}' with default value 0.5")
                            logger.error(f"üîÆ PREDICT: Added missing meta feature '{col}' with default value 0.5")
                    
                    # Keep only the expected columns in the right order
                    meta_df = meta_df[self.meta_feature_names]
                    
                    # Critical check for lstm_prob
                    if 'lstm_prob' not in meta_df.columns:
                        print("üîÆ PREDICT: Critical error: 'lstm_prob' column is missing!")
                        logger.error("üîÆ PREDICT: Critical error: 'lstm_prob' column is missing!")
                        meta_df['lstm_prob'] = 0.5
                        print("üîÆ PREDICT: Added missing 'lstm_prob' column with default value 0.5")
                        logger.error("üîÆ PREDICT: Added missing 'lstm_prob' column with default value 0.5")
                    
                    # Scale meta-features
                    meta_scaled = self.meta_scaler.transform(meta_df)
                    print("üîÆ PREDICT: Scaled meta-features")
                    logger.error("üîÆ PREDICT: Scaled meta-features")
                    
                    # Use meta-model to get final prediction
                    if hasattr(self.meta_model, 'predict_proba'):
                        final_prob = float(self.meta_model.predict_proba(meta_scaled)[:, 1][0])
                        print(f"üîÆ PREDICT: Meta-model probability: {final_prob:.4f}")
                        logger.error(f"üîÆ PREDICT: Meta-model probability: {final_prob:.4f}")
                    else:
                        final_prob = float(self.meta_model.predict(meta_scaled)[0])
                        print(f"üîÆ PREDICT: Meta-model prediction: {final_prob}")
                        logger.error(f"üîÆ PREDICT: Meta-model prediction: {final_prob}")
                except Exception as e:
                    print(f"üîÆ PREDICT: Error in meta-model prediction: {e}")
                    print(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
                    logger.error(f"üîÆ PREDICT: Error in meta-model prediction: {e}")
                    logger.error(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
                    
                    # Fall back to simple averaging
                    final_prob = sum(model_predictions.values()) / len(model_predictions)
                    print(f"üîÆ PREDICT: Using average of model predictions as fallback: {final_prob:.4f}")
                    logger.error(f"üîÆ PREDICT: Using average of model predictions as fallback: {final_prob:.4f}")
            else:
                # Simple averaging if no meta-model
                final_prob = sum(model_predictions.values()) / len(model_predictions)
                print(f"üîÆ PREDICT: No meta-model available, using average: {final_prob:.4f}")
                logger.error(f"üîÆ PREDICT: No meta-model available, using average: {final_prob:.4f}")
            
            # Convert probability to class label (0 or 1)
            prediction = 1 if final_prob >= 0.5 else 0
            
            print(f"üîÆ PREDICT: Final prediction: {prediction} with probability {final_prob:.4f}")
            logger.error(f"üîÆ PREDICT: Final prediction: {prediction} with probability {final_prob:.4f}")
            
            return final_prob, prediction
                
        except Exception as e:
            print(f"üîÆ PREDICT ERROR: Error in ensemble prediction: {e}")
            print(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üîÆ PREDICT ERROR: Error in ensemble prediction: {e}")
            logger.error(f"üîÆ PREDICT ERROR TRACEBACK:\n{traceback.format_exc()}")
            return 0.5, 0  # Default values
    
    def _get_top_features(self, n=10):
        """
        Get top n features by importance across all models.
        
        Args:
            n: Number of top features to return
            
        Returns:
            list: Top n feature names
        """
        try:
            print(f"üìä TOP: _get_top_features called for top {n} features")
            logger.error(f"üìä TOP: _get_top_features called for top {n} features")
            
            if not self.feature_importance:
                print("üìä TOP: No feature importance data available")
                logger.error("üìä TOP: No feature importance data available")
                return []
                
            # Combine feature importance from all models
            combined_importance = defaultdict(float)
            
            # Sum importance across models
            for model_name, importances in self.feature_importance.items():
                print(f"üìä TOP: Adding importance from {model_name}")
                logger.error(f"üìä TOP: Adding importance from {model_name}")
                for feature, importance in importances.items():
                    combined_importance[feature] += importance
            
            # Sort features by importance
            sorted_features = sorted(combined_importance.items(), key=lambda x: x[1], reverse=True)
            
            # Get top n features
            top_n = [f[0] for f in sorted_features[:n]]
            
            print(f"üìä TOP: Top {len(top_n)} features: {top_n}")
            logger.error(f"üìä TOP: Top {len(top_n)} features: {top_n}")
            
            # Return top n feature names
            return top_n
            
        except Exception as e:
            print(f"üìä TOP ERROR: Error getting top features: {e}")
            print(f"üìä TOP ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üìä TOP ERROR: Error getting top features: {e}")
            logger.error(f"üìä TOP ERROR TRACEBACK:\n{traceback.format_exc()}")
            return []
            
    def get_model_info(self):
        """
        Get information about the trained models.
        
        Returns:
            dict: Model information
        """
        try:
            print("‚ÑπÔ∏è INFO: get_model_info called")
            logger.error("‚ÑπÔ∏è INFO: get_model_info called")
            
            info = {
                'base_models': list(self.models.keys()),
                'meta_model': type(self.meta_model).__name__ if self.meta_model else None,
                'lstm_model': 'LSTM' if self.lstm_model is not None else None,
                'feature_subsets': {k: len(v) for k, v in self.feature_subsets.items()},
                'total_features': len(self.all_feature_cols) if self.all_feature_cols else 0,
                'metrics': self.model_metrics,
                'top_features': self._get_top_features(10)
            }
            
            print(f"‚ÑπÔ∏è INFO: Model info collected successfully with {len(info['base_models'])} base models")
            logger.error(f"‚ÑπÔ∏è INFO: Model info collected successfully with {len(info['base_models'])} base models")
            
            return info
            
        except Exception as e:
            print(f"‚ÑπÔ∏è INFO ERROR: Error getting model info: {e}")
            print(f"‚ÑπÔ∏è INFO ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"‚ÑπÔ∏è INFO ERROR: Error getting model info: {e}")
            logger.error(f"‚ÑπÔ∏è INFO ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'error': str(e)}
            
    def predict_trend(self, data, symbol=None):
        """
        Predict trend (increase/decrease) for the given data.
        
        Args:
            data: DataFrame with market data
            symbol: Optional symbol for logging
            
        Returns:
            dict: Prediction results
        """
        try:
            print(f"üìà TREND: predict_trend called for {symbol if symbol else 'unknown'}")
            logger.error(f"üìà TREND: predict_trend called for {symbol if symbol else 'unknown'}")
            
            # Run the debug method to help diagnose issues
            self.debug_meta_features()
            
            if data is None or len(data) == 0:
                print("üìà TREND: No data provided")
                logger.error("üìà TREND: No data provided")
                return {'error': 'No data provided'}
                
            # Make prediction
            probability, prediction = self.predict_with_ensemble(data)
            
            # Convert to trend
            trend = "INCREASE" if prediction == 1 else "DECREASE"
            
            # Log prediction
            print(f"üìà TREND: Prediction for {symbol if symbol else 'unknown'}: {trend}")
            print(f"üìà TREND: Confidence: {probability:.4f}")
            logger.error(f"üìà TREND: Prediction for {symbol if symbol else 'unknown'}: {trend}")
            logger.error(f"üìà TREND: Confidence: {probability:.4f}")
            
            result = {
                'symbol': symbol,
                'trend': trend,
                'probability': float(probability),
                'prediction': int(prediction),
                'timestamp': datetime.now().isoformat()
            }
            
            print(f"üìà TREND: Returning prediction result: {result}")
            logger.error(f"üìà TREND: Returning prediction result: {result}")
            
            return result
            
        except Exception as e:
            print(f"üìà TREND ERROR: Error predicting trend: {e}")
            print(f"üìà TREND ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üìà TREND ERROR: Error predicting trend: {e}")
            logger.error(f"üìà TREND ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'error': str(e)}
            
    def calculate_feature_correlation(self, data, top_n=20):
        """
        Calculate correlation between features and target.
        
        Args:
            data: DataFrame with features and target
            top_n: Number of top correlated features to return
            
        Returns:
            dict: Correlation information
        """
        try:
            print(f"üîÑ CORR: calculate_feature_correlation called for top {top_n} features")
            logger.error(f"üîÑ CORR: calculate_feature_correlation called for top {top_n} features")
            
            if 'Label' not in data.columns:
                print("üîÑ CORR: Label column not found in data")
                logger.error("üîÑ CORR: Label column not found in data")
                return {'error': 'Label column not found'}
                
            # Get feature columns
            feature_cols = [col for col in data.columns if col in self.all_feature_cols]
            
            if not feature_cols:
                print("üîÑ CORR: No matching feature columns found")
                logger.error("üîÑ CORR: No matching feature columns found")
                return {'error': 'No matching feature columns found'}
                
            # Calculate correlation with Label
            correlations = {}
            for col in feature_cols:
                if data[col].dtype in [np.float64, np.int64, np.float32, np.int32]:
                    corr = data[col].corr(data['Label'])
                    if not np.isnan(corr):
                        correlations[col] = corr
            
            # Sort by absolute correlation
            sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
            
            # Get top N correlated features
            top_correlated = sorted_corr[:top_n]
            
            print(f"üîÑ CORR: Found {len(top_correlated)} correlated features")
            logger.error(f"üîÑ CORR: Found {len(top_correlated)} correlated features")
            
            result = {
                'top_correlated_features': dict(top_correlated),
                'positive_correlated': dict(sorted(
                    [(k, v) for k, v in sorted_corr if v > 0],
                    key=lambda x: x[1], reverse=True
                )[:top_n]),
                'negative_correlated': dict(sorted(
                    [(k, v) for k, v in sorted_corr if v < 0],
                    key=lambda x: x[1]
                )[:top_n])
            }
            
            print(f"üîÑ CORR: Calculated correlation results successfully")
            logger.error(f"üîÑ CORR: Calculated correlation results successfully")
            
            return result
            
        except Exception as e:
            print(f"üîÑ CORR ERROR: Error calculating feature correlation: {e}")
            print(f"üîÑ CORR ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üîÑ CORR ERROR: Error calculating feature correlation: {e}")
            logger.error(f"üîÑ CORR ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'error': str(e)}
    
    def detect_market_regime(self, data, n_regimes=None):
        """
        Detect market regime from data.
        
        Args:
            data: DataFrame with market data
            n_regimes: Optional override for number of regimes
            
        Returns:
            dict: Detected market regime information
        """
        try:
            print("üîç REGIME: detect_market_regime called")
            logger.error("üîç REGIME: detect_market_regime called")
            
            if n_regimes is None:
                n_regimes = self.n_regimes
            
            # Simple regime detection based on volatility and trend
            if 'Close' not in data.columns:
                print("üîç REGIME: Close column not found in data")
                logger.error("üîç REGIME: Close column not found in data")
                return {'error': 'Close column not found'}
            
            # Calculate returns
            returns = data['Close'].pct_change().dropna()
            
            if len(returns) < 10:
                print("üîç REGIME: Not enough data for regime detection")
                logger.error("üîç REGIME: Not enough data for regime detection")
                return {'error': 'Not enough data for regime detection'}
            
            # Calculate volatility (20-day rolling standard deviation)
            volatility = returns.rolling(window=20).std().dropna()
            
            # Calculate trend (20-day returns)
            trend = returns.rolling(window=20).mean().dropna()
            
            if len(volatility) == 0 or len(trend) == 0:
                print("üîç REGIME: Not enough data for volatility/trend calculation")
                logger.error("üîç REGIME: Not enough data for volatility/trend calculation")
                return {'error': 'Not enough data for volatility/trend calculation'}
            
            # Get current values
            current_volatility = volatility.iloc[-1]
            current_trend = trend.iloc[-1]
            
            # Categorize regime
            if current_trend > 0 and current_volatility < volatility.quantile(0.7):
                regime = "Bull Market (Low Volatility)"
                regime_id = 0
            elif current_trend > 0 and current_volatility >= volatility.quantile(0.7):
                regime = "Bull Market (High Volatility)"
                regime_id = 1
            elif current_trend <= 0 and current_volatility < volatility.quantile(0.7):
                regime = "Bear Market (Low Volatility)"
                regime_id = 2
            else:
                regime = "Bear Market (High Volatility)"
                regime_id = 3
            
            print(f"üîç REGIME: Detected regime: {regime}")
            logger.error(f"üîç REGIME: Detected regime: {regime}")
            
            result = {
                'regime': regime,
                'regime_id': regime_id,
                'volatility': float(current_volatility),
                'trend': float(current_trend),
                'avg_volatility': float(volatility.mean()),
                'max_volatility': float(volatility.max()),
                'timestamp': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            print(f"üîç REGIME ERROR: Error detecting market regime: {e}")
            print(f"üîç REGIME ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üîç REGIME ERROR: Error detecting market regime: {e}")
            logger.error(f"üîç REGIME ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'error': str(e)}
    
    def analyze_feature_importance(self, data=None, top_n=20):
        """
        Analyze feature importance across models.
        
        Args:
            data: Optional DataFrame for calculating correlation with target
            top_n: Number of top features to return
            
        Returns:
            dict: Feature importance analysis
        """
        try:
            print(f"üìä FEATURE: analyze_feature_importance called for top {top_n} features")
            logger.error(f"üìä FEATURE: analyze_feature_importance called for top {top_n} features")
            
            # Get top features from model importance
            top_features = self._get_top_features(top_n)
            
            # Calculate correlation if data is provided
            correlation_results = None
            if data is not None and 'Label' in data.columns:
                correlation_results = self.calculate_feature_correlation(data, top_n)
            
            # Get feature importance per model
            per_model_importance = {}
            for model_name, importances in self.feature_importance.items():
                # Sort by importance
                sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)
                # Get top N
                per_model_importance[model_name] = dict(sorted_importances[:top_n])
            
            print(f"üìä FEATURE: Analyzed feature importance across {len(per_model_importance)} models")
            logger.error(f"üìä FEATURE: Analyzed feature importance across {len(per_model_importance)} models")
            
            result = {
                'top_features': top_features,
                'per_model_importance': per_model_importance,
                'correlation_results': correlation_results,
                'timestamp': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            print(f"üìä FEATURE ERROR: Error analyzing feature importance: {e}")
            print(f"üìä FEATURE ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üìä FEATURE ERROR: Error analyzing feature importance: {e}")
            logger.error(f"üìä FEATURE ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'error': str(e)}
    
    def get_model_metrics(self):
        """
        Get metrics for all models.
        
        Returns:
            dict: Model metrics
        """
        try:
            print("üìè METRICS: get_model_metrics called")
            logger.error("üìè METRICS: get_model_metrics called")
            
            if not self.model_metrics:
                print("üìè METRICS: No model metrics available")
                logger.error("üìè METRICS: No model metrics available")
                return {'error': 'No model metrics available'}
            
            # Enhanced metrics summary
            metrics_summary = {
                'meta_learner': self.model_metrics.get('meta_learner', {}),
                'ensemble': self.model_metrics.get('ensemble', {}),
                'lstm': self.model_metrics.get('lstm', {}),
                'base_models': {
                    model_name: metrics 
                    for model_name, metrics in self.model_metrics.items() 
                    if model_name not in ['meta_learner', 'ensemble', 'lstm']
                }
            }
            
            # Add overall best model by different metrics
            if len(self.model_metrics) > 0:
                best_models = {}
                
                # Find best model by each metric
                for metric in ['accuracy', 'f1_score', 'f2_score', 'auc', 'precision', 'recall']:
                    best_model = max(self.model_metrics.items(), 
                                    key=lambda x: x[1].get(metric, 0) 
                                    if isinstance(x[1], dict) else 0)
                    best_models[f'best_{metric}'] = {
                        'model': best_model[0],
                        'value': best_model[1].get(metric, 0) if isinstance(best_model[1], dict) else 0
                    }
                
                metrics_summary['best_models'] = best_models
            
            print(f"üìè METRICS: Retrieved metrics for {len(self.model_metrics)} models")
            logger.error(f"üìè METRICS: Retrieved metrics for {len(self.model_metrics)} models")
            
            return metrics_summary
            
        except Exception as e:
            print(f"üìè METRICS ERROR: Error getting model metrics: {e}")
            print(f"üìè METRICS ERROR TRACEBACK:\n{traceback.format_exc()}")
            logger.error(f"üìè METRICS ERROR: Error getting model metrics: {e}")
            logger.error(f"üìè METRICS ERROR TRACEBACK:\n{traceback.format_exc()}")
            return {'error': str(e)}